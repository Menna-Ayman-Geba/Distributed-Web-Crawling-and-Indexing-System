# Distributed-Web-Crawling-and-Indexing-System
This project focuses on building a distributed web crawling and indexing system utilizing cloud computing technologies. Developed in Python 3.9 , the system leverages cloud-hosted virtual machines for scalable and efficient distributed processing. It employs distributed task queues and storage to crawl websites and construct a searchable index of web pages. A major emphasis is placed on fault tolerance, ensuring the system remains operational and continues processing even in the event of partial failures.

## Needed liabraries
On Ubuntu EC2 instance the following dependencies are needed:
```bash
awscli             1.40.0
beautifulsoup4     4.13.4
boto3              1.38.1
botocore           1.38.1
certifi            2025.1.31
charset-normalizer 3.4.1
colorama           0.4.6
docutils           0.19
idna               3.10
jmespath           1.0.1
mpi4py             3.1.4
pip                25.0
pyasn1             0.6.1
python-dateutil    2.9.0.post0
PyYAML             6.0.2
requests           2.32.3
rsa                4.7.2
s3transfer         0.12.0
setuptools         75.8.0
six                1.17.0
soupsieve          2.7
typing_extensions  4.13.2
urllib3            1.26.20
wheel              0.45.1
Whoosh             2.7.4
```
## Demo link
https://drive.google.com/drive/folders/1-QefOENL5haTV3GFhrOninAfil2PmJSm?usp=drive_link
