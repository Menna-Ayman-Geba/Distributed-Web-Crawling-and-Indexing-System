# Distributed-Web-Crawling-and-Indexing-System
This project focuses on building a distributed web crawling and indexing system utilizing cloud computing technologies. Developed in Python, the system leverages cloud-hosted virtual machines for scalable and efficient distributed processing. It employs distributed task queues and storage to crawl websites and construct a searchable index of web pages. A major emphasis is placed on fault tolerance, ensuring the system remains operational and continues processing even in the event of partial failures.


##steps to run code:
1.make sure you installed the follwing liabraries
-pip install mpi4py
-pip install redis 
-pip install whoosh (will be needed in indexer node)
-pip install scrapy (will be needed in crawler node)


